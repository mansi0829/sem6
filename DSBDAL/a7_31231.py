# -*- coding: utf-8 -*-
"""A7_31231.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x4Blj75dtrGqCGele1qPMV-_FXmQCbSm
"""

import nltk
from nltk.tokenize import sent_tokenize,word_tokenize

text="""Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.
The sky is pinkish-blue. You shouldn't eat cardboard"""

tokenized_text=sent_tokenize(text)
print(tokenized_text)

tokenized_word=word_tokenize(text)
print(tokenized_word)

from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word)
print(fdist)

fdist.most_common(2)

import matplotlib.pyplot as plt
fdist.plot(30 , cumulative=False)
plt.show()

#display stop word

from nltk.corpus import stopwords
stop_words=set(stopwords.words("english"))
print(stop_words)

filtered_sent=[]
for w in tokenized_word:
    if w not in stop_words:
        filtered_sent.append(w)

print("tokenized word: ", tokenized_word)
print("after filtering", filtered_sent)

#steeming
from nltk.stem import PorterStemmer

ps = PorterStemmer()

stemmed_words=[]
for w in filtered_sent:
    stemmed_words.append(ps.stem(w))

print("Filtered Sentence:",filtered_sent)
print("Stemmed Sentence:",stemmed_words)

#Lexicon Normalization steeming difference
from nltk.stem.wordnet import WordNetLemmatizer
lem = WordNetLemmatizer()

from nltk.stem.porter import PorterStemmer
stem = PorterStemmer()

word = "flying"
print("Lemmatized Word:",lem.lemmatize(word,"v"))
print("Stemmed Word:",stem.stem(word))

lem_words=[]
for w in filtered_sent:
    lem_words.append(lem.lemmatize(w,"v"))

print("lemmatized word", lem_words )

nltk.pos_tag(filtered_sent)

# frequency matrix for words in each sentence
def _create_frequency_matrix(tokenized_text):
    frequency_matrix = {}
    stopWords = set(stopwords.words("english"))
    ps = PorterStemmer()

    for sent in tokenized_text:
        freq_table = {}
        words = word_tokenize(sent)
        for word in words:
            word = word.lower()
            word = ps.stem(word)
            if word in stopWords:
                continue

            if word in freq_table:
                freq_table[word] += 1
            else:
                freq_table[word] = 1

        frequency_matrix[sent[:15]] = freq_table

    return frequency_matrix

freq_matrix = _create_frequency_matrix(tokenized_text)
print(freq_matrix)

tf_matrix = {}

for sent, f_table in freq_matrix.items():
        tf_table = {}

        count_words_in_sentence = len(f_table)
        for word, count in f_table.items():
            tf_table[word] = count / count_words_in_sentence

        tf_matrix[sent] = tf_table

print(tf_matrix)



word_per_doc_table = {}

for sent, f_table in freq_matrix.items():
    for word, count in f_table.items():
        if word in word_per_doc_table:
            word_per_doc_table[word] += 1
        else:
            word_per_doc_table[word] = 1

print(word_per_doc_table)

import math

idf_matrix = {}

for sent, f_table in freq_matrix.items():
    idf_table = {}

    for word in f_table.keys():
        idf_table[word] = math.log10(len(tokenized_text) / float(word_per_doc_table[word]))

    idf_matrix[sent] = idf_table

print(idf_matrix)

tf_idf_matrix = {}

for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):

    tf_idf_table = {}

    for (word1, value1), (word2, value2) in zip(f_table1.items(),f_table2.items()):  # here, keys are the same in both the table
        tf_idf_table[word1] = float(value1 * value2)

    tf_idf_matrix[sent1] = tf_idf_table

print(tf_idf_matrix)



